{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Classification_with_inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE_Ii3PQ8DzI"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykbEGfxT70jZ"
      },
      "source": [
        "import argparse\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import time\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "from sklearn import metrics\r\n",
        "from collections import Counter\r\n",
        "from sklearn import cluster\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import transforms, datasets\r\n",
        "from torch.utils.data import DataLoader, random_split\r\n",
        "from torch import IntTensor\r\n",
        "from torch.autograd import Variable\r\n",
        "from torchvision import models\r\n",
        "\r\n",
        "# from train_emotion6 import train_emotion6\r\n",
        "# from models.pretrained_resnet152 import pretrained_resnet152\r\n",
        "# from normalization import normalization_parameter, Normalize\r\n",
        "# from classplot import class_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJiQsNoIJ_WI"
      },
      "source": [
        "# 정규화 하는 함수\r\n",
        "\r\n",
        "def normalization_parameter(dataloader):\r\n",
        "    mean = 0.\r\n",
        "    std = 0.\r\n",
        "    nb_samples = len(dataloader.dataset)\r\n",
        "    # tqdm은 진행상태를 알려주는 함수\r\n",
        "    for data,_ in tqdm(dataloader):\r\n",
        "        batch_samples = data.size(0)\r\n",
        "        data = data.view(batch_samples, data.size(1), -1)\r\n",
        "        mean += data.mean(2).sum(0)\r\n",
        "        std += data.std(2).sum(0)\r\n",
        "    mean /= nb_samples\r\n",
        "    std /= nb_samples\r\n",
        "    return mean.numpy(),std.numpy()\r\n",
        "\r\n",
        "class Normalize(object):\r\n",
        "    def __init__(self, mean, std):\r\n",
        "        self.mean = mean\r\n",
        "        self.std = std\r\n",
        "\r\n",
        "    def __call__(self, tensor):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\r\n",
        "        Returns:\r\n",
        "            Tensor: Normalized image.\r\n",
        "        \"\"\"\r\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\r\n",
        "            #print(t)\r\n",
        "            t.sub_(m)\r\n",
        "            t.div_(s)\r\n",
        "            # The normalize code -> t.sub_(m).div_(s)\r\n",
        "        return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PWFB0x8J7Od"
      },
      "source": [
        "# 사진이 레이블링이 잘 됐나, 사진이 잘 나왔나 확인\r\n",
        "def class_plot(data, encoder, inv_normalize=None, n_figures=12, random_seed=42):\r\n",
        "\r\n",
        "    random.seed(random_seed)\r\n",
        "    \r\n",
        "    n_row = int(n_figures/4)\r\n",
        "    fig,axes = plt.subplots(figsize=(14, 10), nrows = n_row, ncols=4)\r\n",
        "    for ax in axes.flatten():\r\n",
        "        a = random.randint(0,len(data))\r\n",
        "        (image,label) = data[a]\r\n",
        "        print(type(image))\r\n",
        "        label = int(label)\r\n",
        "        l = encoder[label]\r\n",
        "        if(inv_normalize!=None):\r\n",
        "            image = inv_normalize(image)\r\n",
        "        \r\n",
        "        image = image.numpy().transpose(1,2,0)\r\n",
        "        im = ax.imshow(image)\r\n",
        "        ax.set_title(l)\r\n",
        "        ax.axis('off')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJCCcTIsBGoq"
      },
      "source": [
        "class pretrained_resnet152(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, freeze=True, n_classes=12):\r\n",
        "        super(pretrained_resnet152, self).__init__()\r\n",
        "\r\n",
        "        self.pretrained = models.resnet152(pretrained=True)\r\n",
        "\r\n",
        "        if freeze:\r\n",
        "            for param in self.pretrained.parameters():\r\n",
        "                param.requires_grad = False\r\n",
        "\r\n",
        "        n_inputs = self.pretrained.fc.out_features\r\n",
        "\r\n",
        "        self.l1 = nn.Linear(n_inputs, 1024)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.dropout = nn.Dropout(0.4)\r\n",
        "        self.l2 = nn.Linear(1024, n_classes)\r\n",
        "        self.LogSoftmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        x = self.pretrained(input)\r\n",
        "        x = x.view(x.size(0), -1) \r\n",
        "        x = self.l1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        # x = self.dropout(x)       \r\n",
        "        x = self.l2(x)\r\n",
        "        x = self.LogSoftmax(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vbbWg_5Be0Q"
      },
      "source": [
        "random_seed = None\r\n",
        "\r\n",
        "# torch.manual_seed(random_seed)\r\n",
        "# torch.cuda.manual_seed(random_seed)\r\n",
        "# torch.backends.cudnn.benchmark = False\r\n",
        "# np.random.seed(random_seed)\r\n",
        "# random.seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JptYsfnMCFsY"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ0mZOdWCR3I"
      },
      "source": [
        "imsize = 300\r\n",
        "batch_size=128\r\n",
        "test_size = 0.1\r\n",
        "epochs = 200\r\n",
        "lr = 0.001\r\n",
        "patience = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sqZ11idDa_8"
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize(size=(imsize, imsize)),\r\n",
        "                                                      transforms.ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxFBX8StCTF3"
      },
      "source": [
        "all_data = datasets.ImageFolder(root='') # dataset root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD8ydfBKIHcA"
      },
      "source": [
        "len(all_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvoPxPNkDaXa"
      },
      "source": [
        "test_data_len = int(len(all_data) * test_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7RMacZ7EXwg"
      },
      "source": [
        "train_data_len = len(all_data) - test_data_len * 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFjEm_yrEc8g"
      },
      "source": [
        "train_data, valid_data, test_data = random_split(all_data, [train_data_len, test_data_len, test_data_len])\r\n",
        "print('len(train_data) : {}, len(valid_data) : {}, len(test_data) : {}'.format(len(train_data), len(valid_data), len(test_data))) # 각 데이터 길이 확인"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X0ZAVwQEpzD"
      },
      "source": [
        "train_data.dataset.transform = transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWxAmLpoFH5n"
      },
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB1_cB12Eolo"
      },
      "source": [
        "mean, std = normalization_parameter(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqVbOBrvSDmX"
      },
      "source": [
        "print(mean, std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCa7YHLxE67w"
      },
      "source": [
        "classes = all_data.classes\r\n",
        "print('classes =', classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdcydadrFSvT"
      },
      "source": [
        "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\r\n",
        "                                                                  transforms.RandomRotation(degrees=10),\r\n",
        "                                                                  transforms.Resize(size=(imsize, imsize)),\r\n",
        "                                                                  transforms.ToTensor(),\r\n",
        "                                                                  transforms.Normalize(mean, std)])\r\n",
        "\r\n",
        "test_transforms = transforms.Compose([transforms.Resize(size=(imsize, imsize)),\r\n",
        "                                                                  transforms.ToTensor(),\r\n",
        "                                                                  transforms.Normalize(mean, std)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "achGkkhHF1eK"
      },
      "source": [
        "train_data.dataset.transform = train_transforms\r\n",
        "valid_data.dataset.transform = test_transforms\r\n",
        "test_data.dataset.transform = test_transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhcDBPVQpDKB"
      },
      "source": [
        "test_data.dataset.transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGPgt-sbGAgI"
      },
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\r\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\r\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEye3NPtGPFL"
      },
      "source": [
        "dataloaders = {'train' : train_loader, 'valid' : valid_loader, 'test' : test_loader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XahMQx2KGYVc"
      },
      "source": [
        "decoder = {}\r\n",
        "for i in range(len(classes)):\r\n",
        "  decoder[classes[i]] = i\r\n",
        "\r\n",
        "print(decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujDhbhB4GZGv"
      },
      "source": [
        "encoder = {}\r\n",
        "for i in range(len(classes)):\r\n",
        "  encoder[i] = classes[i]\r\n",
        "\r\n",
        "print(encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4xCvR4BG7oP"
      },
      "source": [
        "inv_normalize = transforms.Normalize(\r\n",
        "    mean=-1 * np.divide(mean, std),\r\n",
        "    std = 1 / std\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI_h7Ut9HFkH"
      },
      "source": [
        "class_plot(train_data, encoder, inv_normalize=inv_normalize, random_seed=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAb8O_jBKPyI"
      },
      "source": [
        "def train_model(train_class, model, dataloaders, criterion, encoder, num_epochs=10, lr=0.001, batch_size=32, patience=None, classes=None, rgb=False, inv_normalize=None):\r\n",
        "  train_class.model = model\r\n",
        "  train_class.lr = lr\r\n",
        "  \r\n",
        "  dataloader_train = {}\r\n",
        "  losses = list()\r\n",
        "  accuracy = list()\r\n",
        "  key = dataloaders.keys()\r\n",
        "\r\n",
        "  for phase in key:\r\n",
        "    if (phase == 'test'):\r\n",
        "      perform_test = True\r\n",
        "    else:\r\n",
        "      dataloader_train.update([(phase, dataloaders[phase])])\r\n",
        "\r\n",
        "  train_losses, train_acc, valid_losses, valid_acc = train_class.train(dataloader_train, criterion, num_epochs, batch_size, patience)\r\n",
        "\r\n",
        "  train_class.error_plot(valid_losses)\r\n",
        "  train_class.acc_plot(valid_acc)\r\n",
        "\r\n",
        "  if (perform_test == True):\r\n",
        "    true, pred, image, true_wrong, pred_wrong, epoch_acc, epoch_loss = train_class.test(dataloaders['test'], criterion, batch_size) \r\n",
        "\r\n",
        "    train_class.wrong_plot(12, true_wrong, image, pred_wrong, encoder, inv_normalize=inv_normalize)\r\n",
        "    train_class.performance_matrix(true, pred)\r\n",
        "\r\n",
        "    if classes != None:\r\n",
        "      train_class.plot_confusion_matrix(true, pred, classes=classes, title='Confusion matrix, without normalization')\r\n",
        "\r\n",
        "  return train_class.model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB_cbSKhGkgo"
      },
      "source": [
        "classifier = pretrained_resnet152(freeze=True, n_classes=len(classes)).to(device)\r\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FimIVA0Yhgq"
      },
      "source": [
        "# from normalization import Normalize\n",
        "\n",
        "\n",
        "class train_emotion6:\n",
        "    def __init__(self, random_seed=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mean (float): ===========================================================\n",
        "            std (float): ============================================================\n",
        "            attention_list (list): Receive a list of colors you want to give attention\n",
        "            random_seed (int): Seed to fix the result\n",
        "        \"\"\"\n",
        "\n",
        "        if random_seed is not None:\n",
        "            torch.manual_seed(random_seed)\n",
        "            torch.cuda.manual_seed(random_seed)\n",
        "            #torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
        "            #torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "            np.random.seed(random_seed)\n",
        "            random.seed(random_seed)\n",
        "\n",
        "        self.model = None\n",
        "        self.lr = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.earlystop = None\n",
        "\n",
        "    # train\n",
        "    def train(self, dataloaders, criterion, num_epochs, batch_size, patience):\n",
        "        self.model.to(self.device)\n",
        "        best_acc = 0.0\n",
        "        phases = dataloaders.keys()\n",
        "\n",
        "        train_losses = list()\n",
        "        train_acc = list()\n",
        "        valid_losses = list()\n",
        "        valid_acc = list()\n",
        "\n",
        "        # EarlyStopping\n",
        "        if (patience != None):\n",
        "            self.earlystop = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "            print('----------')\n",
        "            optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "            # if (epoch % 10 == 0):\n",
        "            #     self.lr *= 0.9\n",
        "\n",
        "            for phase in phases:\n",
        "                # Train\n",
        "                if phase == 'train':\n",
        "\n",
        "                    self.model.train()\n",
        "                    running_loss = 0.0\n",
        "                    running_corrects = 0\n",
        "                    total = 0\n",
        "                    j = 0\n",
        "\n",
        "                    for batch_idx, (data, target) in enumerate(dataloaders[phase]):\n",
        "                        data, target = Variable(data), Variable(target)\n",
        "                        data = data.type(torch.cuda.FloatTensor)\n",
        "                        target = target.type(torch.cuda.LongTensor)\n",
        "                        optimizer.zero_grad()\n",
        "                        output = self.model(data)\n",
        "                        loss = criterion(output, target)\n",
        "                        _, preds = torch.max(output, 1)\n",
        "                        running_corrects = running_corrects + torch.sum(preds == target.data)\n",
        "                        running_loss += loss.item() * data.size(0)\n",
        "                        j = j + 1\n",
        "\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                                                                                                 \n",
        "                    epoch_acc = running_corrects.double() / (len(dataloaders[phase])*batch_size)\n",
        "                    epoch_loss = running_loss / (len(dataloaders[phase])*batch_size)\n",
        "\n",
        "                    train_losses.append(epoch_loss)\n",
        "                    train_acc.append(epoch_acc)\n",
        "                \n",
        "                # Valid\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        self.model.eval()\n",
        "                        running_loss = 0.0\n",
        "                        running_corrects = 0\n",
        "                        total = 0\n",
        "                        j = 0\n",
        "\n",
        "                        for batch_idx, (data, target) in enumerate(dataloaders[phase]):\n",
        "                            data, target = Variable(data), Variable(target)\n",
        "                            data = data.type(torch.cuda.FloatTensor)\n",
        "                            target = target.type(torch.cuda.LongTensor)\n",
        "                            optimizer.zero_grad()\n",
        "                            output = self.model(data)\n",
        "\n",
        "                            loss = criterion(output, target)\n",
        "                            _, preds = torch.max(output, 1)\n",
        "                            running_corrects = running_corrects + torch.sum(preds == target.data)\n",
        "                            running_loss += loss.item() * data.size(0)\n",
        "                            j = j + 1\n",
        "\n",
        "                        epoch_acc = running_corrects.double() / (len(dataloaders[phase])*batch_size)\n",
        "                        epoch_loss = running_loss / (len(dataloaders[phase])*batch_size)\n",
        "\n",
        "                        valid_losses.append(epoch_loss)\n",
        "                        valid_acc.append(epoch_acc)\n",
        "\n",
        "                print('{} Epoch: {}\\tLoss: {:.6f} \\tAcc: {:.6f}'.format(phase, epoch, running_loss / (j * batch_size), running_corrects.double() / (j * batch_size)))\n",
        "                \n",
        "                if phase == 'valid' and (patience != None):\n",
        "                    self.earlystop(epoch_loss, self.model)  # early stop with valid loss \n",
        "\n",
        "            # print('EalryStop :', self.earlystop.early_stop)\n",
        "\n",
        "            if (patience != None) and (self.earlystop.early_stop):\n",
        "                print(\"Early stopping\")\n",
        "                self.model.load_state_dict(torch.load('./gdrive/MyDrive/Colab Notebooks/dataset/checkpoint.pt'))\n",
        "                break\n",
        "\n",
        "            # print('{} Accuracy: '.format(phase),epoch_acc.item())\n",
        "            print()\n",
        "\n",
        "        return train_losses, train_acc, valid_losses, valid_acc\n",
        "\n",
        "    # test\n",
        "    def test(self, dataloader, criterion, batch_size):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            running_corrects = 0\n",
        "            running_loss = 0\n",
        "            pred = []\n",
        "            true = []\n",
        "            pred_wrong = []\n",
        "            true_wrong = []\n",
        "            image = []\n",
        "            # sm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "            for batch_idx, (data, target) in enumerate(dataloader):\n",
        "                data, target = Variable(data), Variable(target)\n",
        "                data = data.type(torch.cuda.FloatTensor)\n",
        "                target = target.type(torch.cuda.LongTensor)\n",
        "                output = self.model(data)\n",
        "\n",
        "                # need attention here\n",
        "                \n",
        "                loss = criterion(output, target)\n",
        "                # output = sm(output)\n",
        "                _, preds = torch.max(output, 1)\n",
        "                running_corrects = running_corrects + torch.sum(preds == target.data)\n",
        "                running_loss += loss.item() * data.size(0)\n",
        "                preds = preds.cpu().numpy()\n",
        "                target = target.cpu().numpy()\n",
        "                preds = np.reshape(preds, (len(preds), 1))\n",
        "                target = np.reshape(target, (len(preds), 1))\n",
        "                data = data.cpu().numpy()\n",
        "\n",
        "                for i in range(len(preds)):\n",
        "                    pred.append(preds[i])\n",
        "                    true.append(target[i])\n",
        "                    if(preds[i] != target[i]):\n",
        "                        pred_wrong.append(preds[i])\n",
        "                        true_wrong.append(target[i])\n",
        "                        image.append(data[i])\n",
        "\n",
        "            epoch_acc = running_corrects.double()/(len(dataloader)*batch_size)\n",
        "            epoch_loss = running_loss/(len(dataloader)*batch_size)\n",
        "\n",
        "            print(epoch_acc, epoch_loss)\n",
        "\n",
        "            return true, pred, image, true_wrong, pred_wrong, epoch_acc, epoch_loss\n",
        "\n",
        "    def error_plot(self, loss):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(loss)\n",
        "        plt.title(\"Valid loss plot\")\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def acc_plot(self, acc):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(acc)\n",
        "        plt.title(\"Valid accuracy plot\")\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"accuracy\")\n",
        "        plt.show()\n",
        "\n",
        "    # To plot the wrong predictions given by model\n",
        "    def wrong_plot(self, n_figures, true, ima, pred, encoder, inv_normalize):\n",
        "        print('Classes in order Actual and Predicted')\n",
        "        n_row = int(n_figures/3)\n",
        "        fig, axes = plt.subplots(figsize=(14, 10), nrows=n_row, ncols=3)\n",
        "        for ax in axes.flatten():\n",
        "            a = random.randint(0, len(true)-1)\n",
        "\n",
        "            image, correct, wrong = ima[a], true[a], pred[a]\n",
        "            image = torch.from_numpy(image)\n",
        "            correct = int(correct)\n",
        "            c = encoder[correct]\n",
        "            wrong = int(wrong)\n",
        "            w = encoder[wrong]\n",
        "            f = 'A:'+c + ',' + 'P:'+w\n",
        "            if inv_normalize != None:\n",
        "                image = inv_normalize(image)\n",
        "            image = image.numpy().transpose(1, 2, 0)\n",
        "            im = ax.imshow(image)\n",
        "            ax.set_title(f)\n",
        "            ax.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred, classes,\n",
        "                            normalize=False,\n",
        "                            title=None,\n",
        "                            cmap=plt.cm.Blues):\n",
        "        \"\"\"\n",
        "        This function prints and plots the confusion matrix.\n",
        "        Normalization can be applied by setting `normalize=True`.\n",
        "        \"\"\"\n",
        "        if not title:\n",
        "            if normalize:\n",
        "                title = 'Normalized confusion matrix'\n",
        "            else:\n",
        "                title = 'Confusion matrix, without normalization'\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "        # Only use the labels that appear in the data\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            print(\"Normalized confusion matrix\")\n",
        "        else:\n",
        "            print('Confusion matrix, without normalization')\n",
        "\n",
        "        print(cm)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        ax.figure.colorbar(im, ax=ax)\n",
        "        # We want to show all ticks...\n",
        "        ax.set(xticks=np.arange(cm.shape[1]),\n",
        "            yticks=np.arange(cm.shape[0]),\n",
        "            # ... and label them with the respective list entries\n",
        "            xticklabels=classes, yticklabels=classes,\n",
        "            title=title,\n",
        "            ylabel='True label',\n",
        "            xlabel='Predicted label')\n",
        "\n",
        "        # Rotate the tick labels and set their alignment.\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "                rotation_mode=\"anchor\")\n",
        "\n",
        "        # Loop over data dimensions and create text annotations.\n",
        "        fmt = '.2f' if normalize else 'd'\n",
        "        thresh = cm.max() / 2.\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax.text(j, i, format(cm[i, j], fmt),\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        fig.tight_layout()\n",
        "        return ax\n",
        "\n",
        "    def performance_matrix(self, true, pred):\n",
        "        precision = metrics.precision_score(true, pred, average='macro')\n",
        "        recall = metrics.recall_score(true, pred, average='macro')\n",
        "        accuracy = metrics.accuracy_score(true, pred)\n",
        "        f1_score = metrics.f1_score(true, pred, average='macro')\n",
        "        print('Precision: {} Recall: {}, Accuracy: {}: ,f1_score: {}'.format(\n",
        "            precision* 100, recall* 100, accuracy* 100, f1_score* 100))\n",
        "            \n",
        "\n",
        "# EarlyStopping\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=7, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(\n",
        "                f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), './gdrive/MyDrive/Colab Notebooks/dataset/checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPa_bD8_Gvy4"
      },
      "source": [
        "train_animal = train_emotion6(random_seed=random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKqwL1MMG2YY"
      },
      "source": [
        "model = train_model(train_animal, classifier, dataloaders, criterion=criterion, encoder=encoder, num_epochs=epochs, lr=lr, batch_size=batch_size, patience=patience, classes=classes, inv_normalize=inv_normalize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNxCBnLmXlMb"
      },
      "source": [
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "model = pretrained_resnet152(freeze=True, n_classes=len(classes))\r\n",
        "model.load_state_dict(torch.load('./gdrive/MyDrive/Colab Notebooks/dataset/checkpoint.pt'))\r\n",
        "model.eval()\r\n",
        "\r\n",
        "encoder = {0: 'cactus', 1: 'flower', 2: 'orchid', 3: 'succulent_plant', 4: 'tree'}\r\n",
        "imsize=300\r\n",
        "mean, std = [0.5577097,  0.553816,   0.47808513], [0.23067719, 0.22138993, 0.24652666]\r\n",
        "\r\n",
        "image = Image.open('') # test data root\r\n",
        "\r\n",
        "test_transforms = transforms.Compose([transforms.Resize(size=(imsize, imsize)),\r\n",
        "                                                                  transforms.ToTensor(),\r\n",
        "                                                                  transforms.Normalize(mean, std)])\r\n",
        "\r\n",
        "image_tensor = test_transforms(image).float()\r\n",
        "\r\n",
        "image_tensor = image_tensor.unsqueeze_(0)\r\n",
        "\r\n",
        "input = Variable(image_tensor)\r\n",
        "\r\n",
        "output = model(input)[0].detach().numpy()\r\n",
        "\r\n",
        "# print(output)\r\n",
        "\r\n",
        "lst = output.argsort()\r\n",
        "\r\n",
        "print('1st : {} {:.2f}%'.format(encoder[lst[-1]], np.exp(output[lst[-1]]) * 100))\r\n",
        "print('2nd : {} {:.2f}%'.format(encoder[lst[-2]], np.exp(output[lst[-2]]) * 100))\r\n",
        "print('3rd : {} {:.2f}%'.format(encoder[lst[-3]], np.exp(output[lst[-3]]) * 100))\r\n",
        "print('4th : {} {:.2f}%'.format(encoder[lst[-4]], np.exp(output[lst[-4]]) * 100))\r\n",
        "print('5th : {} {:.2f}%'.format(encoder[lst[-5]], np.exp(output[lst[-5]]) * 100))\r\n",
        "\r\n",
        "plt.imshow(image)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}